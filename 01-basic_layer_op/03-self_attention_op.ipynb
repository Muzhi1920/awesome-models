{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03-self_attention_op.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN+ByAL4isQhxaORmA5mVhr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"WLqN4wE4k-PD"},"source":["## 自注意力机制的实现\n","\n","1. X * X^T 用于计算向量间的相关度，结果为对角阵；\n","2. self_attention，将输入X通过线性变换得到对应Q，K，V；\n","3. 基于Q，K得到自注意力权值，再通过softmax实现归一化权值mm；\n","4. 对V进行加权求和。\n","\n","参考：https://mp.weixin.qq.com/s/G_sU1c3UGfHRZ2FVCT6Ucw"]},{"cell_type":"code","metadata":{"id":"P8sUF9bbkaqX","executionInfo":{"status":"ok","timestamp":1637046661824,"user_tz":-480,"elapsed":2637,"user":{"displayName":"于印霄","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhpVABOiHAVZoTKCbmgFadD0IEDNAgt6qrk6rQQ=s64","userId":"12984923686518792253"}}},"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout, Embedding\n","import numpy as np"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bTWkCmXBli1t"},"source":["输入矩阵\n","- 早：[1,2,1,2,1],\n","- 上：[1,1,3,2,1],\n","- 好：[3,1,2,1,1.0]"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ngsOblq6k_YF","executionInfo":{"status":"ok","timestamp":1637046664844,"user_tz":-480,"elapsed":411,"user":{"displayName":"于印霄","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhpVABOiHAVZoTKCbmgFadD0IEDNAgt6qrk6rQQ=s64","userId":"12984923686518792253"}},"outputId":"6d586c9f-d90e-44e1-ca6e-fba70ef3fd90"},"source":["x=tf.constant([[1,2,1,2,1],[1,1,3,2,1],[3,1,2,1,1.0]])\n","x"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n","array([[1., 2., 1., 2., 1.],\n","       [1., 1., 3., 2., 1.],\n","       [3., 1., 2., 1., 1.]], dtype=float32)>"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LIpKCFxPlF1v","executionInfo":{"status":"ok","timestamp":1637046668834,"user_tz":-480,"elapsed":579,"user":{"displayName":"于印霄","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhpVABOiHAVZoTKCbmgFadD0IEDNAgt6qrk6rQQ=s64","userId":"12984923686518792253"}},"outputId":"62ff419b-2f7d-4c98-9129-01aa83e09486"},"source":["tf.transpose(x)"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(5, 3), dtype=float32, numpy=\n","array([[1., 1., 3.],\n","       [2., 1., 1.],\n","       [1., 3., 2.],\n","       [2., 2., 1.],\n","       [1., 1., 1.]], dtype=float32)>"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"KppSD6JIlo3Z"},"source":["矩阵`M`是一个方阵，，里面保存了每个向量，与自己和其他向量进行内积运算的结果。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Scws5GgmlF4V","executionInfo":{"status":"ok","timestamp":1637046676497,"user_tz":-480,"elapsed":428,"user":{"displayName":"于印霄","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhpVABOiHAVZoTKCbmgFadD0IEDNAgt6qrk6rQQ=s64","userId":"12984923686518792253"}},"outputId":"f58e382f-d73c-4b81-e916-c72d06bb5eb1"},"source":["m = tf.matmul(x,tf.transpose(x))\n","m"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n","array([[11., 11., 10.],\n","       [11., 16., 13.],\n","       [10., 13., 16.]], dtype=float32)>"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"86v5L_RblfaG"},"source":["对角阵，任意两个点积表征两个单位向量的夹角，一个向量在另一个向量上的投影。投影值越大，说明向量相关高；意味着在关注“早”的时候，更多的也会关注到“上”。"]},{"cell_type":"markdown","metadata":{"id":"19n3jquTl3Av"},"source":["## 归一化"]},{"cell_type":"code","metadata":{"id":"US91JwdblF6m"},"source":["mm = tf.nn.softmax(m)\n","mm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sGDz1X-dl8-8"},"source":["1. 在新的向量中，每个维度的数值均是其他词向量在对应维度上的加权求和，这个新的行向量就是\"早\"字的词向量经过注意力机制加权求和之后的表示。\n","2. 每一个字的向量都是，对所有向量进行加权求和；最终每个字都融合了自身和其他字的info"]},{"cell_type":"code","metadata":{"id":"WhcEvgONlOhj"},"source":["tf.matmul(mm, x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4i0CAkb2mDZP"},"source":["## self-attention\n","未直接使用seq_emb作为（X），进行运算，而要对其（X）进行线性变换得到Q，K，V再进行selft-attention\n","\n","1. 为了提升模型的拟合能力；\n","2. `W_q，W_k，W_v`变换矩阵均是可训练，起到一个缓冲的效果。\n","\n","1. 假设Q，K服从均值为0，方差为1的分布；，那么A=Q^T* K的分布为均值为0，方差为d；\n","2. 当d变得很大时，A的方差跟着变大，导致`mm`的分布会趋于陡峭(分布的方差大，分布集中在绝对值大的区域)。\n","\n","归一化后的分布与d有关，处理后方差又变为1，从而使得训练过程中梯度值保持稳定。\n"]},{"cell_type":"code","metadata":{"id":"PFtQiJ1hlF_W"},"source":[""],"execution_count":null,"outputs":[]}]}